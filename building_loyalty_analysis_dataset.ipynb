{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is import data from Parse.ly's AWS bucket for the relevant  time period into Google BigQuery.  \n",
    "The key and passphrase for these are with rachit.kinger@jpimedia.co.uk. More key and passwords can be obtained by reaching out directly to Parse.ly.  \n",
    "This data is best imported via GCP's GUI. Go to `GCP > Storage > Transfer` and the steps from there should be obvious.  \n",
    "Remember to specify file filters to download data only for the relevant months.  \n",
    "\n",
    "Once this data has been download the follow these steps to import into a single 'large' database. DO NOT use this database for analysis. Only around 30% of this dataset is useful for us. This database will have a column called `action` which has mostly two values, `pageview` or `heartbeat`. The `pageview` is what is of interest to us and it occupies only 30% of the database.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following commands were run from bash using gsutil\n",
    "# create empty table with the rawdata schema\n",
    "%%bash\n",
    "bq mk -t --schema /home/rachit/gdrive/GCP/bigquery_parsely/bqtable_from_cli/parsely_rawdata_schema.json \\\n",
    "--time_partitioning_type=DAY --time_partitioning_field ts_action \\\n",
    "--require_partition_filter=TRUE --clustering_fields='apikey,action' \\\n",
    "parsely.rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following commands were run from bash using gsutil\n",
    "# use the following command to load the raw parsely data into the the empty table created in previous step\n",
    "%%bash\n",
    "bq load --source_format=NEWLINE_DELIMITED_JSON --max_bad_records=1000 --ignore_unknown_values parsely.rawdata gs://parsely/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=306244342965-pv1n0vhib5nv2ks66pnj59csdf8cqor0.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery&state=AtIZlfeTclQVuvYzghB6Ym6dYJmFRB&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "# use oauth to authenticate access into GCP and bigquery\n",
    "# toggle this variable depending on whether you are on a machine that has an Internet browser or not\n",
    "launch_browser = True \n",
    "# The `launch_browser` boolean variable indicates if a local server is used\n",
    "# as the callback URL in the auth flow. A value of `True` is recommended,\n",
    "# but a local server does not work if accessing the application remotely,\n",
    "# such as over SSH or from a remote Jupyter notebook.\n",
    "\n",
    "from google_auth_oauthlib import flow\n",
    "\n",
    "# if you are not Rachit Kinger please build your own oauth access client id and key and set the \n",
    "# path in the function below to your own client secret\n",
    "\n",
    "appflow = flow.InstalledAppFlow.from_client_secrets_file(\n",
    "    '/home/rachit/gdrive/GCP/oauth_client_key/data-team-rachit-kinger-desktop-apps.json',\n",
    "    scopes = ['https://www.googleapis.com/auth/bigquery'])\n",
    "\n",
    "if launch_browser:\n",
    "    appflow.run_local_server()\n",
    "else:\n",
    "    appflow.run_console()\n",
    "    \n",
    "credentials = appflow.credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access bigquery\n",
    "from google.cloud import bigquery\n",
    "project_id = \"bigquery-test-165213\"\n",
    "client = bigquery.Client(project = project_id, credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results loaded to table /projects/bigquery-test-165213/datasets/parsely/tables/raw_data_for_jan_feb\n"
     ]
    }
   ],
   "source": [
    "#create new dataset without the non-pageviews data\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "#set destination table\n",
    "table_ref = client.dataset('parsely').table('raw_data_for_jan_feb')\n",
    "job_config.destination = table_ref\n",
    "\n",
    "sql = '''\n",
    "SELECT\n",
    "  apikey,\n",
    "  event_id,\n",
    "  flags_is_amp,\n",
    "  ip_city,\n",
    "  ip_lat,\n",
    "  ip_lon,\n",
    "  metadata_page_type,\n",
    "  metadata_section,\n",
    "  metadata_tags,\n",
    "  metadata_title,\n",
    "  ref_category,\n",
    "  ref_domain,\n",
    "  session,\n",
    "  session_id, \n",
    "  session_initial_referrer,\n",
    "  ts_action,\n",
    "  ua_device,\n",
    "  ua_devicetype, \n",
    "  url_clean,\n",
    "  visitor,\n",
    "  visitor_site_id,\n",
    "  visitor_network_id\n",
    "FROM\n",
    "  `bigquery-test-165213.parsely.rawdata`\n",
    "WHERE\n",
    "  ts_action < TIMESTAMP(\"2019-03-01\") --FOR Jan, Feb\n",
    "  AND action = \"pageview\"\n",
    "  AND visitor_site_id IN (\"\", \"OPTOUT\")\n",
    "'''\n",
    "\n",
    "#run the query\n",
    "query_job = client.query(\n",
    "sql,\n",
    "# location must match that of the dataset(s) referenced in the query\n",
    "# and that of the destination table\n",
    "location = 'EU',\n",
    "job_config=job_config)\n",
    "\n",
    "query_job.result() # waits for the query to finish\n",
    "print(\"Query results loaded to table {}\".format(table_ref.path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of steps tries to impute session_ids in order to determine how frequently users visit our sites. The main reason why sessions ids are not captured is because AMP pages do not return session data however, Parse.ly does manage to assign `visitor_site_id` to the visitors of AMP pages. To calculate the number of sessions a user has had we will assume that a session lasts maximum for 30 minutes and any pageviews outside of these time gaps are part of different sessions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results loaded to table /projects/bigquery-test-165213/datasets/parsely/tables/imputed_session_ids_for_null_sessions\n"
     ]
    }
   ],
   "source": [
    "# create a separate dataset which has imputed session_ids where session_ids do not exist\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "# assign destination table\n",
    "table_ref = client.dataset('parsely').table('imputed_session_ids_for_null_sessions')\n",
    "job_config.destination = table_ref\n",
    "\n",
    "sql = '''\n",
    "  /*\n",
    "loyalty analysis\n",
    "creating table 1\n",
    "where session_id = NULL\n",
    "where visitor_site_id != BLANK\n",
    "This table will create another table which has the same schema as\n",
    "rawdata_for_loyalty_analysis but will compute session_id for those users who have NULL as session_ids\n",
    "this table will then be unioned with table 2 where session_id != NULL and visitor_site_id != BLANK\n",
    "\n",
    "NOTE: All rows where visitor_site_id IN (\"\", \"OPTOUT\") will be removed for analysis\n",
    "*/\n",
    "\n",
    "SELECT\n",
    "  * EXCEPT (ts_action, session_change,\n",
    "    time_diff,\n",
    "    previous_ts),\n",
    "  SUM(session_change) OVER (PARTITION BY visitor_site_id ORDER BY ts_action ASC) AS session_id\n",
    "FROM (\n",
    "  SELECT\n",
    "    *,\n",
    "    CASE\n",
    "      WHEN time_diff IS NULL THEN 1\n",
    "      WHEN time_diff > 1800 THEN 1\n",
    "      ELSE 0\n",
    "    END AS session_change\n",
    "  FROM (\n",
    "    SELECT\n",
    "      *,\n",
    "      TIMESTAMP_DIFF(ts_action, previous_ts, SECOND) AS time_diff\n",
    "    FROM (\n",
    "      SELECT\n",
    "        visitor_site_id,\n",
    "        event_id,\n",
    "        ts_action,\n",
    "        LAG(ts_action) OVER (PARTITION BY visitor_site_id ORDER BY ts_action ASC) AS previous_ts\n",
    "      FROM\n",
    "        `bigquery-test-165213.parsely.raw_data_for_jan_feb`\n",
    "      WHERE\n",
    "        session_id IS NULL) AS w_prev_ts) AS w_time_diff) AS w_sess_change\n",
    "'''\n",
    "\n",
    "# run the query\n",
    "query_job = client.query(\n",
    "sql,\n",
    "# location must match that of the dataset(s) referenced in the query\n",
    "# and that of the destination table\n",
    "location = 'EU',\n",
    "job_config=job_config)\n",
    "\n",
    "query_job.result() # waits for the query to finish\n",
    "print(\"Query results loaded to table {}\".format(table_ref.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results loaded to table /projects/bigquery-test-165213/datasets/parsely/tables/jan_feb_with_session_ids\n"
     ]
    }
   ],
   "source": [
    "# reinserting imputed session ids back into raw_data_for_jan_feb\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "table_ref = client.dataset('parsely').table('jan_feb_with_session_ids')\n",
    "job_config.destination = table_ref\n",
    "\n",
    "sql = '''\n",
    "  /*\n",
    "Inserting imputed session_ids for null sessions back into\n",
    "raw data\n",
    "Using LEFT JOIN on rawdata with imputed rawdata*/\n",
    "\n",
    "SELECT\n",
    "  main.* EXCEPT(session_id),\n",
    "  CASE\n",
    "    WHEN main.session_id IS NULL THEN imputed.session_id\n",
    "    ELSE main.session_id\n",
    "  END AS session_id\n",
    "FROM (\n",
    "  SELECT\n",
    "    *\n",
    "  FROM\n",
    "    `bigquery-test-165213.parsely.raw_data_for_jan_feb`) AS main\n",
    "LEFT JOIN (\n",
    "  SELECT\n",
    "    event_id,\n",
    "    session_id\n",
    "  FROM\n",
    "    `bigquery-test-165213.parsely.imputed_session_ids_for_null_sessions`) AS imputed\n",
    "ON\n",
    "  main.event_id = imputed.event_id\n",
    "'''\n",
    "\n",
    "# run the query\n",
    "query_job = client.query(\n",
    "sql,\n",
    "# location must match that of the dataset(s) referenced in the query\n",
    "# and that of the destination table\n",
    "location = 'EU',\n",
    "job_config=job_config)\n",
    "\n",
    "query_job.result() # waits for the query to finish\n",
    "print(\"Query results loaded to table {}\".format(table_ref.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine monthly loyalty segment for each visitor\n",
    "# add a new column to an existing table and update values based on ts_action\n",
    "# the other option is to separate the jan & feb data into two tables\n",
    "# one for jan, the other for feb and separately put them back into the loyaly_analysis_dataset\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
